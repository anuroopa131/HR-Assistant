Prerequisites:Before running the project, ensure the following are installed on your system:
1.Python 3.9+
2.Node.js + npm
3.Ollama (Required to run Llama 3.2B locally)
4.Git
5.(Optional) GPU support for faster inference


Install and Set Up Ollama:This project uses Llama 3.2B model locally through Ollama.
Step 1 — Install Ollama
Download based on your OS:
Windows / macOS / Linux → https://ollama.com/download
Step 2 — Pull the model
After installation, open a terminal and run: ollama pull llama3.2


Backend Setup (Django)
1.Create a virtual environment:python -m venv venv
2.Activate it:venv\Scripts\activate
3.Install dependencies:pip install -r requirements.txt
4.Run migrations:python manage.py migrate
5.Start the Django server:python manage.py runserver


Frontend Setup (React)
Install dependencies:npm install
Start the React app:npm start


How the System Works?
1.React frontend → sends user query → Django backend
2.Backend → sends the query to Ollama
3.Ollama → runs Llama 3.2B locally → returns response
4.Backend → returns response to frontend → displayed in UI

Additional Instructions:
1.Keep Ollama running in the background while using the app
2.Make sure backend and frontend both run simultaneously
3.Use requirements.txt strictly to avoid version mismatch
4.For heavy documents or large queries, first load may be slightly slower
